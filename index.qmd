\pagestyle{fancy}
\setcounter{page}{7}
\clearpage

# General introduction


```{=tex}
\vspace*{\fill}\par
\pagebreak
```

## Prediction of outcome risk {.unnumbered}

A patient's baseline risk---her or his probability of experiencing an outcome of
interest---is a crucial component of medical decision making. For example, the
patients' baseline 10-year cardiovascular is essential in the European Society
of Cardiology and the European Society of Hypertension guidelines of 2018 for
the management of arterial hypertension [@williams20182018]. Similarly, an
algorithm for management of osteoporosis has been suggested, based on a
patient's osteoporotic fracture risk [@kanis2019algorithm].

Risk prediction models are mathematical functions relating the presence of the
outcome of interest to a set of measured predictors (covariates). These models
are important tools for the assessment of a patient's baseline risk
[@moons2012risk]. The performance of a prediction model in new patients is
crucial. Model performance is often expressed by its discrimination, i.e. its
ability to separate lower from higher risk patients, and its calibration, i.e.
the agreement of predicted risk to observed event rates
[@steyerberg2019clinical]. Although a risk prediction model may perform well in
terms of discrimination and calibration for risk, it is not necessarily helpful
for medical decision making. Baseline risk is one of the crucial pieces required
for predicting individual responses to treatment. Knowledge of the patients'
responsiveness to treatment, their vulnerability to side-effects and their
preferences for other relevant outcomes is necessary information required for
making truly informed clinical decisions [@kravitz2004evidence]. Predicting more
individualized responses to treatment is the main challenge of this thesis.

## Prediction of treatment effect {.unnumbered}

In order to provide optimal medical care, doctors are advised to align their
clinical practice with the results of well-conducted clinical trials, or the
aggregated results from multiple such trials [@greenfield2007heterogeneity].
This approach implicitly assumes that all patients eligible for treatment
experience the same effects---benefits and harms---of treatment as the reference
trial population. However, the estimated treatment effect is often an average of
heterogeneous treatment effects and, as such, may not be applicable to most
patient subgroups, let alone individual patients. If a treatment causes a
serious adverse event, then treating all patients on the basis of an observed
average positive treatment effect may be harmful for some [@rothwell1995can].

Heterogeneity of treatment effect is the variation of treatment effects on the
individual level across the population [@kravitz2004evidence]. The
identification and quantification of heterogeneity of treatment effect is
crucial for guiding medical decision making and lies at the core of
patient-centered outcomes research. Despite heterogeneity of treatment effect
being widely anticipated, its evaluation is not straightforward. Individual
treatment effects are---by their nature---unobservable: the moment a patient
receives a specific treatment, their response under the alternatives becomes
unmeasurable (the "fundamental problem of causal inference"
[@holland1986statistics]).

To evaluate a specific individual's response under an alternative treatment,
researchers need to observe the outcomes of other "similar" patients that
actually received the alternative treatment. More individualized treatment
effects are often derived from the average effects estimated within a subgroup
of similar patients. However, patient similarity is not straightforward to
assess. Patients differ in a vast number of characteristics which may or may not
be relevant to modifying treatment responses (the "reference class problem"
[@kahneman1993timid]. Identification of such patient characteristics is
challenging. In clinical trials it usually relies on the detection of
statistically significant (pre-specified) interactions of treatment with
measured covariates (subgroup analyses).

As clinical trials are in general only adequately powered to detect an overall
treatment effect of a certain size, subgroup analyses can be highly problematic.
Lack of statistical power often results in falsely concluding "consistency" of
the treatment effect across several subpopulations of interest or overestimating
the effect size of a treatment-covariate interaction. The former because an
existing interaction effect was smaller than the detectable effect size, the
latter because of false positives introduced from multiple testing. In
@fig-power the statistical power for detecting an interaction effect of equal
size to the main effect is below 30%, despite the clinical trial being powered
at 80% for the detection of the overall effect. Existing guidance on carrying
out subgroup analyses attempts to mitigate these issues
[@guyatt1993users;@sun2014how;@rothwell2005subgroup].

```{r fig-power, cache=TRUE, echo=FALSE, fig.cap="Statistical power for the detection of an interaction when the interaction effect size is between 0 and 4 times the main effect size. For simplicity, we assume equal number of treated and untreated patients and that patients are equally separated between the subgroup levels", fig.show="hold", out.width = '100%'}
grid::grid.raster(tiff::readTIFF(here::here("figures/PowerPlot.tiff")))
```

## Prediction of treatment effect using outcome risk {.unnumbered}

Baseline risk is an important determinant of treatment effect
[@kent2010assessing;@kent2018personalized]. It sets an upper bound on the
treatment effect size. Low risk patients can only experience minimal treatment
benefit before their risk is reduced to zero, while high risk patients can
benefit much more (@fig-riskBenefit). Consequently, baseline risk can be used as
a subgrouping variable for assessing heterogeneity of treatment effect. For many
populations of patients for whom we aim to estimate treatment effects,
well-performing models for predicting baseline risk already exist and can be
used to stratify the patients into subgroups
[@conroy2003estimation;@lindstr_om2003diabetes]. If no such models exist, the
researcher can develop one in the dataset that is used for treatment effect
estimation
[@kent2010assessing;@burke2014using;@kent2019predictive1;@kent2019predictive2;@abadie2018endogenous].

```{r fig-riskBenefit, cache=TRUE, echo=FALSE, fig.cap="Statistical power for the detection of an interaction when the interaction effect size is between 0 and 4 times the main effect size. For simplicity, we assume equal number of treated and untreated patients and that patients are equally separated between the subgroup levels", out.width = '100%'}
grid::grid.raster(tiff::readTIFF(here::here("figures/riskBenefit.tiff")))
```

Baseline risk can also be used directly for predicting individual treatment
benefit [@califf1997selection;@dahabreh2016using]. For example Califf et al
[@califf1997selection] predicted individual benefits regarding 30-day mortality
with tissue plasminogen activator (tPA) compared to streptokinase treatment in
patients with acute myocardial infarction using baseline mortality risk and
assuming a constant relative tPA treatment effect. However, relative treatment
effect does not need to be constant. Modeling more flexible interactions of
treatment with baseline outcome risk may provide more accurate absolute benefit
predictions for individual patients.

Depending on the scale treatment effect is measured on, heterogeneity of
treatment effect may or may not be identified (Figure 1.2). For example, despite
finding statistically significant subgroup effect evaluated on the relative
scale, the absolute risk difference between the two groups may be too small to
have any clinical relevance [@venekamp2014subgroup]. Therefore, in the presence
of a truly effective treatment, effect heterogeneity should always be
anticipated on some scale [@dahabreh2016using], as baseline risk is bound to
vary across trial patients. If effect modifiers are known and the available
sample size provides adequate statistical power for evaluating
treatment-covariate interactions, modeling these interactions would be the
optimal approach for assessing heterogeneity of treatment effect. However, this
approach may lead to overfitting and unstable estimates for the interaction
effects if the aforementioned conditions are not met [@klaveren2018proposed].

```{r fig-scale, cache=TRUE, echo=FALSE, fig.cap="Scale dependency of treatment effect heterogeneity. In the left panel a constant odds ratio of 0.8 is assumed. In the right panel a constant absolute risk reduction of 0.1 is assumed.", fig.show="hold", out.width = '100%'}
grid::grid.raster(tiff::readTIFF(here::here("figures/ScalePlot.tiff")))
```

## Observational data {.unnumbered}

Healthcare data is routinely collected by general practitioners, hospitals,
insurance companies, and many other private or public bodies and is becoming
increasingly available, giving researchers access to massive amounts of patient
data. Theoretically, the aforementioned statistical power challenges for the
evaluation of heterogeneity of treatment effect would be largely mitigated if
the analyses were performed on even a single such database. However, as this
data is not being accumulated for research purposes, it suffers from many biases
that need to be accounted for. Doctors prescribing a specific treatment
expect---usually based on results from clinical trials---that it will be
beneficial for the patient they are treating. Therefore, patients receiving
different treatments tend to differ in several characteristics, often relevant
to their prognosis for the outcomes of interest and, thus, treatment comparisons
cannot be performed directly.

If all patient characteristics on which the treating physician based their
decision have been captured in the observational dataset, methods are available
that can be used to account for these systematic differences
[@rosenbaum1983central;@d_agostino_jr_1998propensity;@austin2011introduction;@dahabreh2012do].
Among the more popular ones is limiting the analyses to the propensity score
matched subpopulation. Propensity scores are the patient-specific probabilities
of receiving the treatment under study and have been shown to have the balancing
property, that is, conditional on the propensity score, treatment assignment is
independent of the potential outcomes [@rosenbaum1983central]. This means that
in a subset of patients with equal propensity scores, covariate distributions do
not differ between patients receiving the treatment under study and those who
are not. Consequently, patients within this subset can be assumed to be
randomized.

Unfortunately, more often than not, a critical amount of the information that
was used for treatment decisions is not captured. As a consequence, propensity
score adjustment will not suffice to evaluate treatment effects using the
observational data, be it overall or subgroup effects. Sensitivity analyses
searching for evidence of this systematic unmeasured imbalances have been
proposed and can be of assistance in many situations
[@schuemie2014interpreting;@ryan2013defining;@schuemie2016robust;@schuemie2018empirical].

Another important problem with observational databases is lack of syntactic and
semantic interoperability. As anyone gathering routinely observed healthcare
data did so in a way that was more convenient to them, a plethora of structures
for the resulting databases arose. Diseases, treatments, medical exams and many
more aspects of healthcare are often coded differently in different
observational databases. In addition, more fundamental disparities between
databases also factor in database incompatibility: different types of
information are recorded in different databases. Different patient
characteristics are captured---at different levels of detail---in a general
practitioner database, in a hospital medical record or in an administrative
claims database. This means that combining results from multiple databases is
not a simple task and requires in-depth understanding of the underlying
healthcare system.

A widely accepted approach to improve interoperability is the use of a common
data model, specifically the Observational Medical Outcomes Partnership Common
Data Model (OMOP-CDM) which is maintained by the Observational Health Data
Sciences and Informatics initiative
[@hripcsak2015observational;@matcho2014fidelity]. The OMOP-CDM standardizes both
the structure and the coding system. Many databases have been mapped to the
OMOP-CDM by the European Health Data and Evidence Network (EHDEN) project in
Europe. With this high level of standardization, the design and execution of
highly scalable observational studies was made possible. Common definitions of
diseases, treatments, and outcomes can now be applied uniformly across a network
of many databases containing information on hundreds of millions of patients. An
analysis plan can be executed following the exact same steps across the network
providing effect estimates derived in different populations. The fragmented
information scattered across multiple databases can now be summarized in a
consistent way to give a fuller picture.

The power of this approach was demonstrated in a large-scale comparative
effectiveness study of first-line treatment for hypertension
[@suchard2019comprehensive]. This study compared five different first-line drug
classes prescribed for hypertension regarding three primary effectiveness, six
secondary effectiveness, and 46 safety outcomes across a global network of nine
observational databases, all mapped to OMOP-CDM. The results complemented the
already available evidence generated in clinical trials, confirming earlier
findings and providing effect estimates on previously unexplored comparisons.

Observational databases provide access to millions of "real-life" patients. This
motivates the exploration of methods for the assessment of treatment effect
heterogeneity in the observational setting despite the challenges inherent to
this type of data. The statistical power problem related to multiple subgroup
analyses can still be present, as observational data is high-dimensional, i.e.,
the number of measured patient characteristics increases with the number of
patients. Attempting a treatment effect modeling approach, where
treatment-covariate interactions are modeled for the prediction of
individualized treatment benefits, suffers from the same statistical power
issues and often results in highly variable estimates. Therefore, using baseline
outcome risk as the subgrouping variable, can provide useful insight into
treatment effect heterogeneity within the observational setting. Modern
libraries for developing risk prediction models and for correcting for
confounding are available and---capitalizing on OMOP-CDM---can be easily applied
across databases with millions of patients.

## Aims {.unnumbered}

The overall aim of this thesis is to explore the use of risk prediction models
as the basis for medical decision making. We will study and apply methods for
the evaluation of heterogeneity of treatment effect in both clinical trial data
and observational data. The specific research aims are:

1.  *Systematically review the current literature on predictive approaches to
treatment effect heterogeneity*. The focus is on regression modeling
approaches applied in clinical trial data.
2.  *Develop scalable and reproducible risk-based predictive approaches to the
assessment of heterogeneity of treatment effect*. We will explore new risk
stratification approaches in observational settings and more individualized
approaches in the clinical trial setting.
3.  *Apply risk-based methods to better guide medical decisions*. We will
develop baseline risk prediction models in several clinical case studies.

In **@sec-review** we present the results of a scoping literature review of
regression modeling approaches for the assessment of treatment effect
heterogeneity in the clinical trial setting. In **@sec-framework** we develop a
standardized scalable framework for the assessment of treatment effect
heterogeneity using a risk-stratified approach in the observational setting. In
**@sec-simulation** we compare different risk-based methods for predicting
individualized treatment effects using extensive simulations of clinical trials.
In **@sec-melanoma** we develop and externally validate a model for the
prediction of 5-year recurrence risk in sentinel node positive melanoma
patients, using data from nine European Organization for Research and Treatment
of Cancer centers. In **@sec-covid** we develop and temporally validate a model
for the prediction of 28-day mortality and admission to the ICU for patients
presenting at the emergency department with suspected COVID-19 infection at four
large Dutch hospitals between March and August, 2020. In **@sec-osteoporosis**
we apply the standardized framework developed in **@sec-framework** to evaluate
effect heterogeneity of teripatide treatment compared to oral bisphosphonates in
female patients above the age of 50 with established osteoporosis. Finally, in
**@sec-discussion** we present a general discussion along with perspectives on
future work.

